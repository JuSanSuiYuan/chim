/* Generated by Chim Compiler - CUDA Backend */
/* Target: NVIDIA CUDA GPU */
/* Compute Capability: sm_70 */

#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <stdio.h>

// CUDA Kernels
__global__ void add(int a, int b) {
    // Thread indexing
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // Shared memory
    __shared__ float shared_data[256];

    auto .tmp2 = b;
    int .tmp3 = .tmp2 + .tmp3;
    // Unsupported instruction
}

__global__ void multiply(int x, int y) {
    // Thread indexing
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // Shared memory
    __shared__ float shared_data[256];

    auto .tmp5 = y;
    int .tmp6 = .tmp5 * .tmp6;
    // Unsupported instruction
}

__global__ void main() {
    // Thread indexing
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // Shared memory
    __shared__ float shared_data[256];

    .tmp9 = add(.tmp7, .tmp8);
    .tmp12 = multiply(.tmp10, .tmp11);
}

// Host code for kernel launch
void launch_kernels() {
    // Launch add kernel
    int numBlocks = (dataSize + 256 - 1) / 256;
    add<<<numBlocks, 256>>>(/* args */);
    cudaDeviceSynchronize();

    // Launch multiply kernel
    int numBlocks = (dataSize + 256 - 1) / 256;
    multiply<<<numBlocks, 256>>>(/* args */);
    cudaDeviceSynchronize();

    // Launch main kernel
    int numBlocks = (dataSize + 256 - 1) / 256;
    main<<<numBlocks, 256>>>(/* args */);
    cudaDeviceSynchronize();

}
